---
import PageLayout from "@/layouts/Base.astro";

const meta = {
	title: "Resources",
	description: "Datasets, codebases, and research papers related to LLM persuasion",
};

type LinkWithUrl =
	| string
	| {
			url: string;
			label?: string;
	  };

type ResourceLink =
	| string
	| {
			url: string;
			description: string;
	  };

type DatasetLink =
	| string
	| {
			url: string | null;
			description: string;
	  };

type ResourceItem = {
	title: string;
	year: string;
	authors: string[];
	description: string;
	paper: LinkWithUrl | null;
	studyWebsite: LinkWithUrl | null;
	dataset: DatasetLink | DatasetLink[] | null;
	code: ResourceLink | null;
};

const getResourceUrl = (link: LinkWithUrl) => (typeof link === "string" ? link : link.url);

const getResourceLabel = (link: LinkWithUrl, fallback: string) =>
	typeof link === "string" ? fallback : (link.label ?? fallback);

const getResourceDescription = (link: ResourceLink | DatasetLink, fallback: string) =>
	typeof link === "string" ? fallback : link.description;

const getResourceHref = (link: DatasetLink) => (typeof link === "string" ? link : link.url);

const getResourceLinks = (links: DatasetLink | DatasetLink[] | null) => {
	if (!links) return [];

	return Array.isArray(links) ? links : [links];
};

const getShortAuthors = (authors: string[]) => {
	if (authors.length === 0) return "";

	const firstAuthor = authors[0]?.trim() ?? "";
	const firstAuthorSurname = firstAuthor.split(" ").filter(Boolean).at(-1) ?? firstAuthor;

	return authors.length > 1 ? `${firstAuthorSurname} et al.` : firstAuthorSurname;
};

const resources: ResourceItem[] = [
	{
		title: "Persuading across Diverse Domains: A Dataset and Persuasion Large Language Model",
		year: "2024",
		authors: [
			"Chuhao Jin",
			"Kening Ren",
			"Lingzhen Kong",
			"Xiting Wang",
			"Ruihua Song",
			"Huan Chen",
		],
		description:
			"Introduces DailyPersuasion, a large-scale multi-domain persuasive dialogue dataset, and PersuGPT, a model specialized in persuasion strategies.",
		paper: "https://aclanthology.org/2024.acl-long.92/",
		studyWebsite: "https://persugpt.github.io",
		dataset: {
			url: "https://github.com/PersuGPT/PersuGPT.github.io/blob/main/DailyPersuasion_full_version.zip",
			description:
				"DailyPersuasion: A dataset covering 13,000 dialogue scenarios across 35 distinct domains.",
		},
		code: {
			url: "https://persugpt.github.io",
			description: "Code for the PersuGPT model and data collection framework.",
		},
	},
	{
		title:
			"Measuring and Benchmarking Large Language Models‚Äô Capabilities to Generate Persuasive Language",
		year: "2024",
		authors: ["Amalie Brogaard Pauli", "Isabelle Augenstein", "Ira Assent"],
		description: "A study of LLM ability to produce persuasive text.",
		paper: "https://arxiv.org/abs/2406.17753",
		studyWebsite: null,
		dataset: {
			url: "https://huggingface.co/datasets/APauli/Persuasive-Pairs",
			description:
				"<strong>Persuasive-Pairs</strong>: 2,700 text pairs from news, debates, and chats. Each includes an LLM-rewritten version with varying persuasiveness, validated by three human annotators.",
		},
		code: {
			url: "https://huggingface.co/APauli/Persuasive_language_in_pairs",
			description:
				"A trained regression model model to evaluate the relative persuasiveness between two text samples.",
		},
	},
	{
		title: "MakeMeSay, OpenAI o3-mini System Card",
		year: "2025",
		authors: ["OpenAI"],
		description: "",
		paper: "https://cdn.openai.com/o3-mini-system-card-feb10.pdf",
		studyWebsite: null,
		dataset: null,
		code: {
			url: "https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say",
			description:
				"This evaluation tests a model‚Äôs ability to generate persuasive or manipulative text, specifically in the setting of a game where one AI system has to get the other party (an AI system simulating a human) to say a specific codeword without arousing suspicion.",
		},
	},
	{
		title: "MakeMePay, OpenAI o3-mini System Card",
		year: "2025",
		authors: ["OpenAI"],
		description: "",
		paper: "https://cdn.openai.com/o3-mini-system-card-feb10.pdf",
		studyWebsite: null,
		dataset: null,
		code: {
			url: "https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay",
			description:
				"This evaluation tests an AI system‚Äôs ability to generate persuasive or manipulative text, specifically in the setting of convincing another (simulated) entity to part with money.",
		},
	},
	{
		title: "Among Them: A Game-Based Framework for Assessing Persuasion Capabilities of LLMs",
		year: "2025",
		authors: [
			"Mateusz Idziejczak",
			"Vasyl Korzavatykh",
			"Mateusz Stawicki",
			"Andrii Chmutov",
			"Marcin Korcz",
			"Iwo B≈ÇƒÖdek",
			"Dariusz Brzezinski",
		],
		description:
			"An evaluation framework using social deduction gameplay to measure how LLMs use persuasion and deception in dynamic environments.",
		paper: "https://arxiv.org/abs/2502.20426",
		studyWebsite: null,
		dataset: {
			url: "https://doi.org/10.5281/zenodo.14935395",
			description:
				"Logs and transcripts from simulated social deduction games used to analyze model behavior.",
		},
		code: {
			url: "https://github.com/Farmerobot/among_them",
			description:
				"A simulation platform inspired by 'Among Us' for testing persuasive and deceptive capabilities in LLMs.",
		},
	},
	{
		title:
			"Persuade Me if You Can: A Framework for Evaluating Persuasion Effectiveness and Susceptibility Among Large Language Models",
		year: "2025",
		authors: ["Nimet Beyza Bozdag", "Shuhaib Mehri", "Gokhan Tur", "Dilek Hakkani-T√ºr"],
		description:
			"The PMIYC framework evaluates LLMs in multi-turn conversations to measure both their effectiveness as persuaders and susceptibility as persuadees.",
		paper: "https://arxiv.org/abs/2503.01829",
		studyWebsite: "https://beyzabozdag.github.io/PMIYC",
		dataset: {
			url: "https://github.com/beyzabozdag/PersuadeMeIfYouCan/tree/main/data",
			description:
				"Benchmark data used to measure shifts in model and user opinions across controversial topics. The dataset comprises 961 subjective claims spanning political, ethical, and social issues sourced from Durmus et al. and the Perspectrum dataset, alongside 817 factual misinformation question-answer pairs adapted from the TruthfulQA benchmark.",
		},
		code: {
			url: "https://github.com/beyzabozdag/PersuadeMeIfYouCan",
			description: "Code for the multi-agent simulation framework.",
		},
	},
	{
		title: "Measuring and Improving Persuasiveness of Large Language Models",
		year: "2024",
		authors: ["Somesh Singh", "Yaman K Singla", "Harini SI", "Balaji Krishnamurthy"],
		description:
			"Introduces PersuasionBench and PersuasionArena to evaluate LLM generative and simulative persuasion capabilities, including the task of 'transsuasion'.",
		paper: "https://arxiv.org/abs/2410.02653",
		studyWebsite: "https://behavior-in-the-wild.github.io/measure-persuasion",
		dataset: {
			url: "https://huggingface.co/datasets/behavior-in-the-wild/PersuasionArena",
			description:
				"The PersuasionArena dataset for evaluating LLM persuasion across different tasks consists of tweet pairs where two tweets from the same account have similar content and were posted in close temporal proximity, but received significantly different engagement (e.g., number of likes). These differences act as a proxy for persuasiveness, allowing models to be trained and evaluated on generating or ranking more persuasive content.",
		},
		code: null,
	},
	{
		title:
			"It‚Äôs the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics",
		year: "2025",
		authors: ["Matthew Kowal", "Jasper Timm", "Jean-Francois Godbout", "others"],
		description:
			"The APE benchmark evaluates the propensity (willingness) of LLMs to attempt persuasion on harmful topics like conspiracies and violence.",
		paper: "https://arxiv.org/abs/2506.02873",
		studyWebsite: null,
		dataset: null,
		code: {
			url: "https://github.com/AlignmentResearch/AttemptPersuadeEval",
			description:
				"APE framework for measuring persuasion attempts. Includes topics, prompts and code for generating a synthetic dataset.",
		},
	},
	{
		title: "MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents",
		year: "2025",
		authors: ["Kunlun Zhu", "Hongyi Du", "Zhaochen Hong", "others"],
		description:
			"Benchmarks multi-agent coordination and competition across scenarios like coding, research, and games (persuasion related tasks: Werewolf, Bargaining).",
		paper: "https://arxiv.org/abs/2503.01935",
		studyWebsite: null,
		dataset: null,
		code: {
			url: "https://github.com/MultiagentBench/MARBLE",
			description:
				"The MARBLE framework for multi-agent collaboration and competition evaluation. ",
		},
	},

	{
		title: "Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction",
		year: "2024",
		authors: ["Suma Bailis", "Jane Friedhoff", "Feiyang Chen"],
		description:
			"A framework for evaluating LLMs via the social deduction game Werewolf, focusing on persuasion, deception, deduction.",
		paper: "https://arxiv.org/abs/2407.13943",
		studyWebsite: null,
		dataset: null,
		code: {
			url: "https://github.com/google/werewolf_arena",
			description: "Code and prompt templates for the Werewolf Arena simulation framework.",
		},
	},
	{
		title: "Measuring the Persuasiveness of Language Models",
		year: "2024",
		authors: [
			"Esin Durmus",
			"Liane Lovitt",
			"Alex Tamkin",
			"Stuart Ritchie",
			"Jack Clark",
			"Deep Ganguli",
		],
		description:
			"An Anthropic study measuring human belief shifts on various topics after reading arguments generated by Claude models.",
		paper: {
			url: "https://www.anthropic.com/news/measuring-model-persuasiveness",
			label: "Blog post",
		},
		studyWebsite: null,
		dataset: {
			url: "https://huggingface.co/datasets/Anthropic/persuasion",
			description:
				"The Persuasion Dataset contains claims and corresponding human-written and model-generated arguments, along with persuasiveness scores.",
		},
		code: null,
	},

	{
		title: "Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good",
		year: "2019",
		authors: [
			"Xuewei Wang",
			"Weiyan Shi",
			"Richard Kim",
			"Yoojung Oh",
			"Sijia Yang",
			"Jingwen Zhang",
			"Zhou Yu",
		],
		description:
			"Crowdsourced persuasion dialogues where a persuader aims to convince a partner to donate to Save the Children; 1,017 conversations (300 with sentence‚Äëlevel persuasion‚Äëact annotations).",
		paper: "https://arxiv.org/abs/1906.06725",
		studyWebsite: "",
		dataset: {
			url: "https://github.com/ohyj1002/persuasionforgood/tree/master/data",
			description:
				"PersuasionForGood (P4G) dataset with AnnotatedData (300) and FullData (1,017) dialogues.",
		},
		code: {
			url: "https://github.com/ohyj1002/persuasionforgood",
			description: "", //code for classifier model?
		},
	},
	{
		title:
			"What makes a convincing argument? Empirical analysis and detecting attributes of convincingness in Web argumentation",
		year: "2016",
		authors: ["Ivan Habernal", "Iryna Gurevych"],
		description:
			"A comprehensive study shift from normative logic to empirical 'convincingness.' It provides 26k natural language explanations for why one argument is better than another, identifying 17 qualitative dimensions like 'no credible evidence,' 'off-topic,' or 'well-thought-of.'",
		paper: "https://aclanthology.org/D16-1129/",
		studyWebsite: null,
		dataset: [
			{
				url: "https://github.com/UKPLab/acl2016-convincing-arguments/tree/master/data",
				description:
					"UKPConvArg1: 16k argument pairs with pairwise labels and a ranking-based version (UKPConvArgRank) for 1k arguments.",
			},
			{
				url: "https://tudatalib.ulb.tu-darmstadt.de/items/4198f487-1b64-463b-a0ae-a678121f391f",
				description:
					"UKPConvArg2: 9k argument pairs annotated with 17 specific reasons/flaws explaining the convincingness of each choice.",
			},
		],
		code: {
			url: "https://github.com/UKPLab/emnlp2016-empirical-convincingness",
			description:
				"Source code for SVM and Bi-LSTM models used to predict qualitative properties and label distributions.",
		},
	},

	// {
	// 	title: "Automatic Argument Quality Assessment - New Datasets and Methods",
	// 	year: "2019",
	// 	authors: ["Assaf Toledo", "Shai Gretz", "Edo Cohen‚ÄëKarlik", "others"],
	// 	description:
	// 		"Actively collected 6.3k individual arguments with quality labels and 14k argument pairs with pairwise convincingness judgments, released by IBM Project Debater.",
	// 	paper: "https://arxiv.org/pdf/1909.01007.pdf",
	// 	studyWebsite: null,
	// 	dataset: {
	// 		url: "https://huggingface.co/datasets/ibm-research/argument_quality_ranking_30k",
	// 		description:
	// 			"IBM-ArgQ (6.3kArgs & 14kPairs). IBM argument quality ranking dataset (related release) with ~30k items.",
	// 	},
	// 	code: null,
	// },
	// {
	// 	title: "Winning-Arguments / ChangeMyView (CMV)",
	// 	year: "2016",
	// 	authors: ["Chenhao Tan", "Vlad Niculae", "Cristian Danescu-Niculescu-Mizil", "Lillian Lee"],
	// 	description:
	// 		"CMV conversations linking arguments to opinion change (Œî) outcomes; includes paired successful vs. matched unsuccessful threads.",
	// 	paper: "http://chenhaot.com/pages/changemyview.html",
	// 	studyWebsite: "https://convokit.cornell.edu/documentation/winning.html",
	// 	dataset: {
	// 		url: "https://chenhaot.com/data/cmv/README.txt",
	// 		description: "Original CMV data and paired argument threads (training/held‚Äëout).",
	// 	},
	// 	code: null,
	// },
	// {
	// 	title:
	// 		"Winning Arguments: Interaction Dynamics and Persuasion Strategies in Good-faith Online Discussions",
	// 	year: "2016",
	// 	authors: ["Chenhao Tan", "Vlad Niculae", "Cristian Danescu-Niculescu-Mizil", "Lillian Lee"],
	// 	description:
	// 		"Conversations scraped from the r/ChangeMyView subreddit tracking good-faith discussions and subsequent user opinion changes (indicated by awarding a Œî). It includes paired successful (persuasive) vs. matched unsuccessful (non-persuasive) threads.",
	// 	paper: "https://arxiv.org/abs/1602.01103",
	// 	studyWebsite: "",
	// 	dataset: {
	// 		url: "https://chenhaot.com/data/cmv/README.txt",
	// 		description:
	// 			"Original CMV data and paired argument threads (training/held‚Äëout) annotated for persuasiveness based on user deltas.",
	// 	},
	// 	code: null,
	// },
	// {
	// 	title: "Webis-Clickbait-17",
	// 	year: "2017",
	// 	authors: [
	// 		"Martin Potthast",
	// 		"Tim Gollub",
	// 		"Matti Wiegmann",
	// 		"Benno Stein",
	// 		"Matthias Hagen",
	// 		"others",
	// 	],
	// 	description:
	// 		"38,517 Twitter teaser posts from 27 US news publishers with 4‚Äëpoint clickbait strength annotations; often used as a proxy for manipulative teaser design.",
	// 	paper: "https://aclanthology.org/C18-1127/",
	// 	studyWebsite: "https://webis.de/data/webis-clickbait-17.html",
	// 	dataset: {
	// 		url: "https://explore.openaire.eu/search/dataset?pid=10.5281%2Fzenodo.3346490",
	// 		description: "Zenodo DOIs for training/test partitions via the Clickbait Challenge 2017.",
	// 	},
	// 	code: null,
	// },
	// {
	// 	title: "Propaganda Techniques Corpus (PTC)",
	// 	year: "2019‚Äì2020",
	// 	authors: [
	// 		"Giovanni Da San Martino",
	// 		"Seunghak Yu",
	// 		"Alberto Barr√≥n-Cede√±o",
	// 		"Rostislav Petrov",
	// 		"Preslav Nakov",
	// 	],
	// 	description:
	// 		"500+ news articles annotated at span level with 18 propaganda techniques; basis for SemEval‚Äë2020 Task 11.",
	// 	paper: "https://arxiv.org/abs/2003.11563",
	// 	studyWebsite: "https://propaganda.math.unipd.it/ptc/",
	// 	dataset: {
	// 		url: "https://service.tib.eu/ldmservice/dataset/propaganda-techniques-corpus--ptc-",
	// 		description:
	// 			"Dataset hub + leaderboards; span identification and technique classification tasks.",
	// 	},
	// 	code: {
	// 		url: "https://huggingface.co/QCRI/PropagandaTechniquesAnalysis-en-BERT",
	// 		description: "Reference models for fine‚Äëgrained propaganda detection (BERT).",
	// 	},
	// },
	{
		title: "ElecDeb60to20",
		year: "2023‚Äì2025",
		authors: ["Pierpaolo Goffredo", "Mariana Chaves Espinoza", "Elena Cabrio", "Serena Villata"],
		description:
			"U.S. presidential debate transcripts (1960‚Äì2020) annotated for logical fallacies at the utterance/span level, plus argumentative components and relations.",
		paper: "https://openreview.net/pdf?id=Js80TDwMfY",
		studyWebsite: null,
		dataset: {
			url: "https://github.com/pierpaologoffredo/ElecDeb60to20",
			description: "Fallacy, argument component, and relationship annotations; debate‚Äëlevel data.",
		},
		code: {
			url: "https://github.com/pierpaologoffredo/FallacyDetection",
			description: "MultiFusion BERT and baselines for fallacy detection/classification.",
		},
	},
	// {
	// 	title: "ElecDeb60-20: A Dataset for Fallacy Detection in Presidential Debates",
	// 	year: "2023",
	// 	authors: ["Pierpaolo Goffredo", "Mariana Chaves Espinoza", "Elena Cabrio", "Serena Villata"],
	// 	description:
	// 		"US presidential debate transcripts (1960‚Äì2020) annotated for 6 types of logical fallacies, including argument components (claims/premises) and their relations.",
	// 	paper: "https://openreview.net/forum?id=Js80TDwMfY",
	// 	studyWebsite: null,
	// 	dataset: {
	// 		url: "https://github.com/pierpaologoffredo/ElecDeb60to20",
	// 		description:
	// 			"The ElecDeb60-20 dataset containing fallacy, argument component, and relationship annotations.",
	// 	},
	// 	code: {
	// 		url: "https://github.com/pierpaologoffredo/FallacyDetection",
	// 		description:
	// 			"Source code for Multi-Fusion BERT models used for fallacy detection and classification.",
	// 	},
	// },
	// {
	// 	title: "Perspectrum",
	// 	year: "2019",
	// 	authors: ["Sihao Chen", "Daniel Khashabi", "Wenpeng Yin", "Chris Callison-Burch", "Dan Roth"],
	// 	description:
	// 		"Claims linked to diverse supporting/opposing perspectives and evidence paragraphs; stance‚Äëaware perspective discovery dataset.",
	// 	paper: "https://danielkhashabi.com/files/2019_perspectrum/2019_perspectrum_naacl.pdf",
	// 	studyWebsite: "https://cogcomp.seas.upenn.edu/perspectrum/",
	// 	dataset: {
	// 		url: "https://github.com/CogComp/perspectrum",
	// 		description: "Data, scripts, and demo for claims, perspectives, and evidence.",
	// 	},
	// 	code: null,
	// },
	// {
	// 	title: "PersuasiveToM",
	// 	year: "2025",
	// 	authors: ["Fangxu Yu", "Lai Jiang", "Shenyi Huang", "Zhen Wu", "Xinyu Dai"],
	// 	description:
	// 		"Benchmark assessing Theory of Mind in persuasive dialogues: tracking desires/beliefs/intentions and applying ToM to select/evaluate persuasion strategies.",
	// 	paper: "https://arxiv.org/abs/2502.21017",
	// 	studyWebsite: null,
	// 	dataset: {
	// 		url: "https://github.com/Yu-Fangxu/PersuasiveToM",
	// 		description: "Benchmark data and evaluation scripts.",
	// 	},
	// 	code: {
	// 		url: "https://github.com/Yu-Fangxu/PersuasiveToM",
	// 		description: "Reference implementation and evaluation runners.",
	// 	},
	// },
	{
		title: "Can Language Models Recognize Convincing Arguments?",
		year: "2024",
		authors: ["Paula Dolores Rescala", "Manoel Horta Ribeiro", "Tiancheng Hu", "Robert West"],
		description:
			"This paper investigates whether large language models (LLMs) can detect convincing arguments and predict user stances based on demographic and belief profiles. ",
		paper: "https://arxiv.org/abs/2404.00750",
		studyWebsite: "",
		dataset: {
			url: "https://zenodo.org/records/13887286",
			description:
				"The dataset extends Durmus and Cardie's (2018) debate.org corpus with user demographics, prior stances on 48 'big issues', and debate transcripts. It includes PoliProp, containing 833 political debates with manually written propositions and 4,871 votes, and PoliIssues, comprising 121 debates on prominent topics with 751 crowdsourced Amazon Mechanical Turk labels for human benchmarking.",
		},
		code: {
			url: "https://github.com/manoelhortaribeiro/debate-gpt-x?tab=readme-ov-file",
			description: "Code for data processing, analysi.",
		},
	},
	// {
	// 	title: "TeleSalesCorpus",
	// 	year: "2025",
	// 	authors: ["‚Äî"],
	// 	description:
	// 		"Synthetic, multi‚Äëturn, goal‚Äëoriented sales dialogues (~2,000 conversations) for studying persuasive tactics in tele‚Äësales settings.",
	// 	paper: null,
	// 	studyWebsite: null,
	// 	dataset: {
	// 		url: null,
	// 		description:
	// 			"Add your release link here; several synthetic sales corpora exist with similar specs.",
	// 	},
	// 	code: null,
	// },
	// {
	// 	title: "CToMPersu",
	// 	year: "2025",
	// 	authors: ["‚Äî"],
	// 	description:
	// 		"Multi‚Äëdomain persuasion scenarios explicitly annotated for information asymmetry and Theory‚Äëof‚ÄëMind variables.",
	// 	paper: null,
	// 	studyWebsite: null,
	// 	dataset: { url: null, description: "Awaiting canonical link/publication." },
	// 	code: null,
	// },
	// {
	// 	title: "EPP4G & ETP4G (PersuasionForGood extensions)",
	// 	year: "2020‚Äì2024",
	// 	authors: ["‚Äî"],
	// 	description:
	// 		"Extended P4G annotations with emotion labels (EPP4G), politeness‚Äëstrategy labels, and empathetic transfer tags (ETP4G).",
	// 	paper: null,
	// 	studyWebsite: null,
	// 	dataset: { url: null, description: "Add links to the extended annotated releases you use." },
	// 	code: null,
	// },
	// {
	// 	title: "ImageArg",
	// 	year: "‚Äî",
	// 	authors: ["‚Äî"],
	// 	description:
	// 		"A multi‚Äëmodal Twitter dataset targeting image persuasiveness in short social posts.",
	// 	paper: null,
	// 	studyWebsite: null,
	// 	dataset: {
	// 		url: null,
	// 		description:
	// 			"Please provide the canonical link for ImageArg; several ‚Äòimage persuasion‚Äô datasets exist under similar names.",
	// 	},
	// 	code: null,
	// },
	// {
	// 	title: "3MT French (Persuasiveness Ratings)",
	// 	year: "‚Äî",
	// 	authors: ["‚Äî"],
	// 	description:
	// 		"Crowd evaluations of persuasiveness for 3‚Äëminute academic presentations by French PhD students.",
	// 	paper: null,
	// 	studyWebsite: null,
	// 	dataset: { url: null, description: "Awaiting authoritative dataset link/paper to finalize." },
	// 	code: null,
	// },
	// {
	// 	title:
	// 		"Introducing the 3MT French dataset to investigate the timing of public speaking judgements",
	// 	year: "2024",
	// 	authors: ["Beatrice Biancardi", "Mathieu Chollet", "Chlo√© Clavel"],
	// 	description:
	// 		"Crowd-sourced persuasiveness evaluations of 3-minute presentations by French PhD students from the 'Ma Th√®se en 180 secondes' competition, rated on a 5-point Likert scale.",
	// 	paper: "https://link.springer.com/article/10.1007/s10579-023-09712-4",
	// 	studyWebsite: null,
	// 	dataset: {
	// 		url: "https://github.com/abarkar/PublicSpeakingAnalysisWithLLMs",
	// 		description:
	// 			"The original dataset contains video/audio evaluations. Barkar et al. provide generated text transcripts (via Whisper) and LLM-based feature extraction in their repository.",
	// 	},
	// 	code: "https://github.com/abarkar/PublicSpeakingAnalysisWithLLMs",
	// },

	// {
	// 	title: "",
	// 	year: "",
	// 	authors: [],
	// 	description: "",
	// 	paper: "",
	// 	studyWebsite: null,
	// 	dataset: {
	// 		url: "",
	// 		description: "",
	// 	},
	// 	code: {
	// 		url: "",
	// 		description: "",
	// 	},
	// },
	// {
	// 	title: "",
	// 	year: "",
	// 	authors: [],
	// 	description: "",
	// 	paper: "",
	// 	studyWebsite: null,
	// 	dataset: {
	// 		url: "",
	// 		description: "",
	// 	},
	// 	code: {
	// 		url: "",
	// 		description: "",
	// 	},
	// },
	// {
	// 	title: "",
	// 	year: "",
	// 	authors: [],
	// 	description: "",
	// 	paper: "",
	// 	studyWebsite: null,
	// 	dataset: {
	// 		url: "",
	// 		description: "",
	// 	},
	// 	code: {
	// 		url: "",
	// 		description: "",
	// 	},
	// },
];

const withBase = (path: string) => `${import.meta.env.BASE_URL.replace(/\/$/, "")}${path}`;
---

<PageLayout meta={meta}>
	<h1 class="title mb-8">Resources</h1>
	<div class="border-accent/60 bg-accent/10 mb-6 rounded-md border px-4 py-3">
		<p class="text-accent text-sm font-semibold">
			‚ö†Ô∏è This resource list is still being updated and expanded.
		</p>
	</div>

	<!-- <section class="prose prose-sm prose-cactus mb-8 max-w-none">
		<p>
			Datasets, codebases, and research related to persuasion and behavior change in LLMs. Each
			entry may include papers, datasets, code, or a combination. Use the badges below each entry to
			jump to specific resources.
		</p>
	</section> -->

	<div class="space-y-8">
		{
			resources.map((resource) => (
				<div class="border-accent/30 bg-opacity-5 rounded-lg border p-6">
					<h3 class="title mb-2 text-lg">{resource.title}</h3>
					{(resource.studyWebsite ||
						resource.paper ||
						resource.year ||
						resource.authors.length > 0) && (
						<div class="mb-4 flex flex-wrap items-center gap-3 text-sm opacity-80">
							{resource.studyWebsite && (
								<a
									href={getResourceUrl(resource.studyWebsite)}
									target="_blank"
									rel="noreferrer"
									class="border-accent/60 text-accent inline-block flex-shrink-0 rounded border px-3 py-1 text-xs font-semibold"
								>
									üîó Website
								</a>
							)}
							{resource.paper && (
								<a
									href={getResourceUrl(resource.paper)}
									target="_blank"
									rel="noreferrer"
									class="border-accent/60 text-accent inline-block flex-shrink-0 rounded border px-3 py-1 text-xs font-semibold"
								>
									üìÑ {getResourceLabel(resource.paper, "Paper")}
								</a>
							)}
							{(resource.year || resource.authors.length > 0) && (
								<p>
									{resource.year}
									{resource.year && resource.authors.length > 0 && " ‚Ä¢ "}
									{getShortAuthors(resource.authors)}
								</p>
							)}
						</div>
					)}
					<p class="mb-4 text-sm">{resource.description}</p>
					<div class="space-y-3">
						{getResourceLinks(resource.dataset).map((datasetLink) => {
							const datasetHref = getResourceHref(datasetLink);

							return (
								<div class="flex items-start gap-3">
									{datasetHref ? (
										<a
											href={datasetHref}
											target="_blank"
											rel="noreferrer"
											class="bg-accent mt-0.5 inline-flex w-28 flex-shrink-0 justify-center rounded px-3 py-1 text-xs font-semibold text-white dark:text-black"
										>
											üìä Dataset
										</a>
									) : (
										<span class="bg-accent mt-0.5 inline-flex w-28 flex-shrink-0 justify-center rounded px-3 py-1 text-xs font-semibold text-white opacity-70 dark:text-black">
											üìä Dataset
										</span>
									)}
									<p
										class="text-sm"
										set:html={getResourceDescription(datasetLink, "Link to the dataset.")}
									/>
								</div>
							);
						})}
						{resource.code && (
							<div class="flex items-start gap-3">
								<a
									href={getResourceUrl(resource.code)}
									target="_blank"
									rel="noreferrer"
									class="bg-accent mt-0.5 inline-flex w-28 flex-shrink-0 justify-center rounded px-3 py-1 text-xs font-semibold text-white dark:text-black"
								>
									üíª Code
								</a>
								<p class="text-sm">
									{getResourceDescription(resource.code, "Link to the code repository.")}
								</p>
							</div>
						)}
					</div>
				</div>
			))
		}
	</div>

	<section class="prose prose-sm prose-cactus mt-12 max-w-none">
		<h3>How to contribute</h3>
		<p>
			If you know of a relevant resource that is missing, there are two ways to add it:
			<ul class="list-inside list-disc">
				<li>
					<strong>Contact us</strong> ‚Äî email us the resource information, details in the <a
						href={withBase("/contact/")}>Contact</a
					> page.
				</li>
				<li>
					<strong>GitHub</strong> ‚Äî add it to the website's <a
						href="https://github.com/aida-ugent/llm-persuasion-safety-hub"
						target="_blank"
						rel="noreferrer">GitHub repository</a
					> and raise a pull request.
				</li>
			</ul>
		</p>
	</section>
</PageLayout>
